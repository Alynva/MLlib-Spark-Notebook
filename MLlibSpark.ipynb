{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xezl4Ox7CVqH"
      },
      "source": [
        "---\n",
        "# MLlib Spark\n",
        "\n",
        "## Aprendizado de Máquina no Apache Spark\n",
        "\n",
        "Alunos:\n",
        "\n",
        "\n",
        "*   **Alisson Nunes** (725862)\n",
        "*   **Lucas Mathaeus** (726561)\n",
        "*   **William Eugênio** (726601)\n",
        "\n",
        "Bacharelado em Ciência da Computação (BCC)<br>\n",
        "Departamento de Computação (DC)<br>\n",
        "Universidade Federal de São Carlos (UFSCar)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (0) Contextualização\n",
        "\n",
        "## Aprendizado de Máquina\n",
        "\n",
        "- .\n",
        "- .\n",
        "\n",
        "## Classificação\n",
        "\n",
        "## Clusterização\n",
        "\n",
        "## Resilient Distributed Dataset (RDD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  (1) Instalação e Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -q pyspark numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#configurando as variaveis de ambiente\n",
        "import os\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"python3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/08/25 23:03:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "22/08/25 23:03:48 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
          ]
        }
      ],
      "source": [
        "#definindo uma sessão Spark para usar o pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "ss = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/08/25 22:36:33 WARN Utils: Your hostname, monitora-Vostro-3583 resolves to a loopback address: 127.0.1.1; using 192.168.0.6 instead (on interface wlo1)\n",
            "22/08/25 22:36:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/monitora/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "22/08/25 22:36:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "22/08/25 22:36:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "22/08/25 22:36:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
          ]
        }
      ],
      "source": [
        "# Criação do contexto Spark\n",
        "sc = ss.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krASYLGNEisA"
      },
      "source": [
        "# (2) Obtenção dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "#instalando o módulo wget\n",
        "#%%capture\n",
        "! pip install -q wget\n",
        "!rm -rf data\n",
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'data/sample_binary_classification_data (1).txt'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Baixando os dados das tabelas de dimensão do data mart Exame\n",
        "import wget\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_binary_classification_data.txt\"\n",
        "wget.download(url, \"data/sample_binary_classification_data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregando o arquivo de dados\n",
        "from pyspark.mllib.util import MLUtils\n",
        "\n",
        "data = MLUtils.loadLibSVMFile(sc, \"data/sample_binary_classification_data.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (3) Tratamento dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PythonRDD[8] at RDD at PythonRDD.scala:53"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training, test = data.randomSplit([0.6, 0.4], seed=11)\n",
        "training.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/08/25 22:38:28 WARN Instrumentation: [8bfb2d8c] Initial coefficients will be ignored! Its dimensions (1, 692) did not match the expected size (1, 692)\n",
            "22/08/25 22:38:28 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
            "22/08/25 22:38:28 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
          ]
        }
      ],
      "source": [
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
        "\n",
        "model = LogisticRegressionWithLBFGS.train(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "\n",
        "metrics = BinaryClassificationMetrics(predictionAndLabels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Area under PR = 1.0\n",
            "Area under ROC = 1.0\n"
          ]
        }
      ],
      "source": [
        "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
        "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (0.0, 0.0), (0.0, 0.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (0.0, 0.0), (1.0, 1.0)]\n"
          ]
        }
      ],
      "source": [
        "print(predictionAndLabels.take(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
